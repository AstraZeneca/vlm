<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Diffusion Instruction Tuning (Lavender)">
  <meta name="keywords" content="multimodal, vision-language, diffusion, instruction tuning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Lavender: Diffusion Instruction Tuning</title>

  <!-- Bulma, Bootstrap, FontAwesome, etc.. -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="./static/css/index.css">

  <link rel="icon" href="https://cdn-icons-png.flaticon.com/512/5886/5886212.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  <!-- jQuery, FontAwesome JS, etc. -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="text/javascript" async
    src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <!-- ===== Hero / Title Section ===== -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title" style="color: #5b4eadd7;">
                Lavender: Diffusion Instruction Tuning
            </h1>              
            <h3 class="subtitle is-4 publication-title" style="margin-bottom: 1rem;">
              <em>Boosting SoTA vision-language model with Stable Diffusion</em>
            </h3>
            <h5 class="subtitle is-5 publication-awards">
              <strong>Preprint available on arXiv</strong>
            </h5>

            <!-- Authors / Affiliations -->
            <div class="is-size-6 publication-authors" style="margin-top: 1rem;">
              <span class="author-block">
                <a href="https://chenjin.netlify.app/" style="color:#008AD7;font-weight:normal;">Chen Jin<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://rt416.github.io/" style="color:#008AD7;font-weight:normal;">Ryutaro Tanno<sup>2</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://uk.linkedin.com/in/amrutha-saseendran" style="color:#008AD7;font-weight:normal;">Amrutha Saseendran<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://tomdiethe.com/" style="color:#008AD7;font-weight:normal;">Tom Diethe<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://uk.linkedin.com/in/philteare" style="color:#008AD7;font-weight:normal;">Philip Teare<sup>1</sup></a>
              </span>
            </div>
            
            <h5 class="subtitle is-5 publication-awards"></h5>
            <span style="font-size: 20px"><sup>1</sup>AstraZeneca &nbsp;&nbsp;<sup>2</sup>Google DeepMind
            </h5>

            <!-- Additional links (arXiv, PDF, Code, etc.) -->
            <div class="columns is-centered">
              <div class="column has-text-centered">
                <div class="publication-links" style="margin-top: 2rem;">
                  <span class="link-block">
                    <a href="#" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="ai ai-arxiv"></i></span>
                      <span>arXiv</span>
                    </a>
                  </span>
                  <!-- <span class="link-block">
                    <a href="#" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fab fa-github"></i></span>
                      <span>Code</span>
                    </a>
                  </span> -->       
                  <span class="link-block">
                    <a href="https://0379ec528faf2066bf.gradio.live" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fas fa-play"></i></span>
                      <span>Demo</span>
                    </a>
                  </span>
                  <!-- <span class="link-block">
                    <a class="button is-normal is-rounded is-dark" style="opacity: 0.6; cursor: not-allowed;">
                      <span class="icon"><i class="fas fa-brain"></i></span>
                      <span>Model (Coming Soon)</span>
                    </a>
                  </span>             -->
                  <span class="link-block">
                    <a class="button is-normal is-rounded is-dark" style="opacity: 0.6; cursor: not-allowed;">
                      <span class="icon"><i class="fas fa-database"></i></span>
                      <span>Model & Dataset (Coming Soon)</span>
                    </a>
                  </span> 
                </div>
              </div>
            </div>

            <!-- Potential short teaser/highlight -->
            <div class="content" style="max-width: 950px; margin: 0 auto;">
              <p>
                <strong>Lavender</strong> (<strong>L</strong>anguage-<strong>a</strong>nd-<strong>V</strong>ision fin<strong>e</strong>-tu<strong>n</strong>ing with <strong>D</strong>iffusion Align<strong>er</strong>) is a simple supervised fine-tuning (SFT) method that boosts the performance of advanced vision-language models (VLMs) by leveraging state-of-the-art image generation models such as Stable Diffusion. 
              </p>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- ===== Teaser / Banner Section ===== -->
  <section class="hero teaser" style="background-color:#efeff081;">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- Title and tagline, remains centered -->
        <h4 class="subtitle has-text-centered" style="font-size:1.1rem;">
          <strong>Diffusion Instruction Tuning</strong> <br />
          <em>“The visual expertise of image generators in Stable Diffusion can be transferred to enhance text generation in SoTA VLMs.”</em>
        </h4>
        <!-- Centered Abstract Container with fixed max width -->
        <div class="abstract-summary" style="max-width: 1920px; margin: 0 auto; text-align: center;">
          <!-- <h2>Abstract</h2> -->
          <p>
            Lavender aligns the text-vision attention in the VLM transformer with the equivalent used by Stable Diffusion during SFT, instead of adapting separate encoders. This alignment enriches the model’s visual understanding and significantly boosts performance across in- and out-of-distribution tasks.
          </p>
          <p>
            <strong>Key highlights:</strong>
          </p>
  
          <!-- Bullet points also centered -->
          <ul style="list-style-position: inside;">
            <li><strong>Significant Gains:</strong> +30% on 20 tasks, +68% on OOD WorldMedQA.</li>
            <li><strong>Data-Efficient:</strong> Needs only 0.13M samples (~2.5% of typical VLM datasets).</li>
            <li><strong>Low Compute:</strong> Finetunes in ~1 day on 8 NVIDIA A10G GPUs.</li>
            <li><strong>Model-Agnostic:</strong> Works with Llama-3.2-11B, MiniCPM-Llama3-v2.5 & more.</li>
            <li><strong>Precise Alignment:</strong> Transfers strong text-vision alignment from Stable Diffusion.</li>
            <li><strong>Open-Source:</strong> Code, data & finetuned models will be available.</li>
          </ul>
  
          <!-- Expand/Collapse Toggle Button -->
          <button onclick="toggleAbstract()" style="margin-top:10px;">
            Read Introduction
          </button>
  
          <!-- Hidden Full Abstract -->
          <div id="full-abstract" style="display: none; margin-top:10px;">
            <p>
              Training frontier foundation models from scratch costs millions of dollars at minimum, requiring hundreds of GPUs and millions to billions of data. This challenge is even more pronounced in multimodal settings: vision-language models (VLMs) often face data scarcity because collecting paired image-text datasets is expensive. A common workaround is to apply supervised fine-tuning (SFT) on a pretrained large language model (LLM), leveraging its abundant text-only pretraining and adjusting bridging layers or additional encoders with limited image-text pairs. However, these methods typically overlook the importance of transformer-level attention alignment within the LLM core---a key component for effectively expanding text-based models into the visual domain.
              <br><br>
              Precise visual-text alignment is crucial for advanced multimodal reasoning. While both VLMs and diffusion models (DMs) process text and images, they diverge in their generation objectives. We observe that DMs, such as Stable Diffusion, which reconstructs images at the pixel level, appear to learn more precise text-vision attention maps than VLMs that are optimised solely for text token generation (Figure 2).
              <br><br>
              In this work, we demonstrate that the high-quality cross-attention maps from these DMs indeed offer a useful target for guiding the text-vision attention in VLMs during SFT, thus improving word-to-region alignment and the overall performance. We introduce <strong>Lavender</strong> (<strong>L</strong>anguage-<strong>a</strong>nd-<strong>V</strong>ision fin<strong>e</strong>-tu<strong>n</strong>ing with <strong>D</strong>iffusion Align<strong>er</strong>), the first framework to directly align VLM transformer attention layers with those of Stable Diffusion (Figure 3). Specifically, during SFT, Lavender transfers diffusion-based attention distributions to VLMs, enhancing core visual-textual interactions. To mitigate catastrophic forgetting, we additionally propose several attention aggregation methods and training strategies that preserve existing VLM competencies.
            </p>
            <a href="https://arxiv.org/abs/XXXX.XXXXX" target="_blank">See Full Paper on ArXiv</a>
          </div>
        </div><!-- end of abstract-summary -->
      </div><!-- end of hero-body -->
    </div><!-- end of container -->
  </section>
  
  <script>
    function toggleAbstract() {
      const fullAbs = document.getElementById('full-abstract');
      if (fullAbs.style.display === 'none') {
        fullAbs.style.display = 'block';
      } else {
        fullAbs.style.display = 'none';
      }
    }
  </script>
  


  
  
  <script>
    function toggleAbstract() {
      const fullAbs = document.getElementById('full-abstract');
      if (fullAbs.style.display === 'none') {
        fullAbs.style.display = 'block';
      } else {
        fullAbs.style.display = 'none';
      }
    }
  </script>
  
  

  <!-- ===== Figures / Key Insights Section ===== -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Key Insights and Figures</h2>
        </div>
      </div>

      <!-- Row of figures -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified">

              <div style="
              display: flex; 
              flex-wrap: wrap; 
              justify-content: center; 
              align-items: center; 
              gap: 20px;">

              <!-- First Figure -->
              <figure style="
                flex: 1 1 600px; 
                max-width: 100%; 
                min-width: 250px; 
                margin: 0; 
                text-align: center;">
                <img src="static/images/grouped_relative_improvement_wide.png" 
                     alt="Lavender Gains" 
                     style="width: 100%; vertical-align: middle;">
                <figcaption>
                  <small>
                    <em>Figure 1. Average Performance on 20 Vision-Language Reasoning Benchmarks (Grouped into 4 Categories).</em>
                  </small>
                </figcaption>
              </figure>
              
              
            </div>
            

            <div style="
              display: flex; 
              flex-wrap: wrap; 
              justify-content: center; 
              align-items: center; 
              gap: 20px;">
              
              <!-- First Figure -->
              <figure style="
                flex: 1 1 600px; 
                max-width: 80%; 
                min-width: 250px; 
                margin: 0; 
                text-align: center;">
                <img src="static/images/motivation_xattn_v2.jpg" 
                     alt="Lavender Gains" 
                     style="width: 100%; vertical-align: middle;">
                <figcaption>
                  <small>
                    <em>Figure 2. Image generation models (Stable Diffusion on the left) exhibit stronger word-to-region attention alignment than VLMs (Open-Flamingo on the right). Per-word average attention maps suggest that diffusion models may be closer to an ideal distribution correlating image regions with textual tokens.</em>
                  </small>
                </figcaption>
              </figure>
            
              <!-- Second Figure -->
              <figure style="
                flex: 1 1 600px; 
                max-width: 80%; 
                min-width: 250px; 
                margin: 0; 
                text-align: center;">
                <img src="static/images/method_highlight_v33.jpg" 
                     alt="Lavender Gains" 
                     style="width: 100%; vertical-align: middle;">
                <figcaption>
                  <small>
                    <em>Figure 3. <strong>Lavender: Diffusion Instruction Tuning.</strong>
                      Lavender uses the text-vision attention maps of a Stable Diffusion Model, \(Attention_{SDM}\), as a guiding objective for the attention of the target vision-language model (VLM), \(Attention_{VLM}\). The Attention Alignment module employs a 3-Layer ConvNet to transform \(Attention_{VLM}\) to match \(Attention_{SDM}\) via an MSE loss, acting as a regularisation term during supervised fine-tuning.</em>
                  </small>
                </figcaption>
              </figure>
            </div>
            
     

          </div>
        </div>
      </div>

    </div>
  </section>


  <!-- ===== Additional / Detailed Methods Section ===== -->
  <section class="section" id="method" style="background-color:#efeff081">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Method</h2>

      <p>
        Lavender is built on the simple idea of harnessing text-to-image (T2I) diffusion models for 
        image-to-text (I2T) generation tasks. We hypothesize that the cross-attention in T2I models 
        is more fine-grained for spatial alignment, while the VLM’s objective is purely next-token 
        prediction, leading to weaker text-region alignment. By adding a mean-squared error (MSE) loss 
        between the VLM and diffusion cross-attention on the same data, we shift the VLM attention distribution 
        closer to an ideal alignment.
      </p>

      <!-- Code snippet or pseudo-code from your LaTeX -->
      <div class="card mb-3">
        <div class="card-body" style="display:flex; flex-wrap:wrap; align-items:center; justify-content:space-between;">
          <div style="flex:1; min-width:300px; margin-right:1rem;">
            <img src="static/images/lavender_fig_4.jpg" alt="fig 3" style="max-width:100%;">
          </div>
        </div>
      </div>

      <p>
        We propose an <strong>Aligner Network</strong> (a few light convolution layers) to transform 
        the raw VLM attention into a distribution that can be directly matched to the Stable Diffusion 
        attention. When used with parameter-efficient finetuning (LoRA), we see strong results 
        without destabilizing the original VLM’s capabilities.
      </p>
    </div>
  </section>


  <!-- ===== Performance / Benchmark Section ===== -->
  <section class="section" id="results">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Performance Across Benchmarks</h2>
        </div>
      </div>

      <div class="content has-text-justified">
        <p>
          We evaluate Lavender on 16 mainstream vision-language benchmarks, covering chart/diagram/document 
          understanding, perception and multi-discipline reasoning, real-world visual tasks, and 
          visual hallucination detection. The figure below shows a typical relative improvement pattern 
          for <em>Lavender Llama-3.2</em>, where we see up to 50% gains over Small Budget-Constrained SOTA.
        </p>
      </div>
        <figure style="text-align: center;">
            <img src="static/images/plot_relative_improvement_over_sota_full.png" alt="Lavender Gains" style="max-width: 100%;">
            <figcaption>
                <small>
                <em>Lavender improves MiniCPM-V-2.5 and Llama-3.2-11B, surpassing Small Budget-Constrained SOTA by up to 50% with minimal finetuning data (~0.13M).</em>
                </small>
            </figcaption>
            </figure>

        <figure style="text-align: center;">
            <img src="static/images/bar_highlight.png" alt="Lavender Gains" style="max-width: 100%;">
            <figcaption>
                <small>
                <em>Among Large SOTA Models, Lavender-Llama-3.2-11B demonstrates comparable performance to certain High-Resource State-of-the-Art Models at least an order of magnitude larger.</em>
                </small>
            </figcaption>
            </figure>

        <!-- First Figure -->
          <figure style="
          flex: 1 1 600px; 
          max-width: 100%; 
          min-width: 250px; 
          margin: 0; 
          text-align: center;">
          <img src="static/images/lm32_relative_improvement_with_offsets_20_benchmarks_v3_lora.png" 
              alt="Lavender Gains" 
              style="width: 100%; vertical-align: middle;">
          <figcaption>
            <small>
              <em>Lavender boosts the cross-attention-equipped Llama-3.2-11B by up to 30% on 19/20 benchmarks, while mitigating catastrophic forgetting.</em>
            </small>
          </figcaption>
        </figure>
      
        <!-- Second Figure -->
        <figure style="
          flex: 1 1 600px; 
          max-width: 100%; 
          min-width: 250px; 
          margin: 0; 
          text-align: center;">
          <img src="static/images/minicpm_relative_improvement_side_by_side_v3.png" 
              alt="Lavender Gains" 
              style="width: 100%; vertical-align: middle;">
          <figcaption>
            <small>
              <em>Lavender enhances the self-attention-only MiniCPM-Llama3-V-2.5 by up to 4% on 16/20 benchmarks despite further fine-tuning on its pre-trained dataset..</em>
            </small>
          </figcaption>
        </figure>

      <div class="content has-text-justified">
        <p>
          We observe that aligning attention with diffusion models also helps reduce visual
          hallucinations, especially on tasks that require domain-specific knowledge. 
        </p>
      </div>

        <figure style="text-align: center;">
            <img src="static/images/bar_wordmedqa_with_annotations.png" alt="Lavender Gains" style="max-width: 100%;">
            <figcaption>
                <small>
                <em>Without tuning on medical dataset, Lavender boosts Llama-3.2-11B's performance on the out-of-distribution benchmark WorldMedQA by 68%.</em>
                </small>
            </figcaption>
            </figure>

    </div>
  </section>


  <!-- ===== Example Qualitative / Visual Demo Section (optional) ===== -->
  <section class="section" style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Qualitative Examples</h2>
      <div class="columns is-centered">
        <div class="column is-full">
          <p class="has-text-centered" style="margin-bottom:2rem;">
            Below are some sample VQA questions comparing <strong>original Llama-3.2-11B</strong> and 
            <strong>Lavender-Llama-3.2-11B</strong>:
          </p>
          
          <!-- Example 1 -->
          <div class="card mb-3">
            <div class="card-body" style="display:flex; flex-wrap:wrap; align-items:center; justify-content:space-between;">
              <div style="flex:1; min-width:300px; margin-right:1rem;">
                <img src="static/images/lavender_visual_highlight.jpg" alt="Example VQA 1" style="max-width:100%;">
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- ===== BibTeX Section ===== -->
  <section class="section" id="bibtex">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">BibTeX</h2>
      <pre><code>
@inproceedings{jin2024diffusioninstr,
  title={Diffusion Instruction Tuning},
  author={Chen Jin and Ryutaro Tanno and ...},
  booktitle={arxiv},
  year={2024}
}
      </code></pre>
    </div>
  </section>


  <!-- ===== Acknowledgement Section ===== -->
  <section class="section" id="ack">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Acknowledgements</h2>
      <p>
        We thank the creators of <a href="https://github.com/CompVis/stable-diffusion" target="_blank">Stable Diffusion</a>, 
        <a href="https://github.com/meta-llama/llama-cookbook" target="_blank">Llama</a>, 
        <a href="https://github.com/OpenBMB/MiniCPM-o" target="_blank">MiniCPM</a> and 
        <a href="https://github.com/mlfoundations/open_flamingo" target="_blank">Open-Flamingo</a> for providing the foundation codes used in Lavender.
        We also appreciate the open-source community around <em>PEFT</em> frameworks which 
        made this project feasible under constrained data and compute resources.
      </p>
      <p>
        Usage and License Notices: Our data and code are for research use only. 
        They are also restricted by the licenses of 
        <em>Llama, Stable Diffusion, and other upstream models</em>. 
        See our GitHub repository for license details.
      </p>
    </div>
  </section>

  <!-- ===== Footer Section (Optional) ===== -->
  <footer class="footer has-text-centered" style="padding:2rem 1.5rem;">
    <div class="content">
      <p>
        <strong>Lavender</strong> by <a href="https://chenjin.netlify.app/" target="_blank">Chen Jin</a> @ <a href="https://github.com/AstraZeneca" target="_blank">
            Centre for AI, Data Science & Artificial Intelligence, BioPharmaceuticals R&D, AstraZeneca, Cambridge, UK</a>.
        </p>
        <p>
        Built upon open-source code from the broader academic community.
      </p>
    </div>
  </footer>

</body>
</html>
